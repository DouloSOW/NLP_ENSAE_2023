Anomaly Detection For NLP: OOD detection

General Context Assessing the quality of natural language generation systems through human annotation is very expensive.
Additionally, human annotation campaigns are time-consuming and include non-reusable human labour. In practice,
researchers rely on automatic metrics as a proxy of quality. In the last decade, many string-based metrics (e.g., BLEU)
have been introduced. However, such metrics usually rely on exact matches and thus, do not robustly handle synonyms.
This metric can be very usefull to discriminate between generated and human text..... For example if you want to find
out that a text has been generated by ChatGPT.

Problem Statement:

Given a dataset $\mathcal{D} = \{\pmb{x_i}, \{\pmb{y_i^s},h(\pmb{x_i},\pmb{y_i^s})\}_{s=1}^S \}_{i=1}^N$ where $\pmb{x}_
i$ is the $i$-th reference text; $\pmb{y}_i^s$ is the $i$-th candidate text generated by the $s$-th NLG system; $N$ is
the number of texts in the dataset and $S$ the number of systems available. The vector $\pmb{x_i} = ({x_1,\cdots,x_M})$
is composed of M tokens (\textit{e.g.}, words or subwords) and $\pmb{y_i^s} = ({y_1^s,\cdots,y_L^s})$ is composed of L
tokens\footnote{The reference and candidate text can be composed of several sentences as it is the case in
summarization.}. The set of tokens (vocabulary) is denoted as $\Omega$, $\mathbf{T}$ denotes the set of possible texts.
$h(\pmb{x_i},\pmb{y_i^s}) \in \mathcal{R}^+$ is the score associated by a human annotator to the candidate text
$\pmb{y}_i^s$ when comparing it with the reference text $\pmb{x_i}$. We aim at building an evaluation metric $f$ such
that $f(\pmb{x_i} ,\pmb{y_i})\in \mathbb{R}^{+}$.

Evaluation of the proposed metrics.

To assess the relevance of an evaluation metric $f$, correlation with human judgment is considered to be one of the most
important criteria. Debate on the relative merits of different correlations for the evaluation of automatic metrics is
ongoing but classical correlation measures are Pearson, Spearman or Kendall tests. Two meta-evaluation strategies are
commonly used: (1) text-level correlation or (2)
system-level correlation.

Your Task:

Benchmark the correlation of existing metrics with human scores. You can start
from [here](https://github.com/PierreColombo/nlg_eval_via_simi_measures). You can work on whatever generation task you
want: translation [4], data2text generation [3] , story generation [2].

Your reads:

[1] Pierre Colombo, Nathan Noiry, Ekhine Irurozki, Stephan Clemencon What are the best systems? New perspectives on NLP
Benchmarking NeurIPS 2022
[2] Cyril Chhun, Pierre Colombo, Fabian Suchanek, Chloe Clavel Of Human Criteria and Automatic Metrics: A Benchmark of
the Evaluation of Story Generation (oral) COLING 2022
[3] Pierre Colombo, Chloé Clavel and Pablo Piantanida. InfoLM: A New Metric to Evaluate Summarization & Data2Text
Generation. Student Outstanding Paper Award (oral) AAAI 2022
[4] Pierre Colombo, Guillaume Staerman, Chloé Clavel, Pablo Piantanida. Automatic Text Evaluation through the Lens of
Wasserstein Barycenters. (oral) EMNLP 2021
[5] Pierre Colombo, Maxime Peyrard, Nathan Noiry, Robert West, Pablo Piantanida. The Glass Ceiling of Automatic
Evaluation in Natural Language Generation
[6] Hamid Jalalzai, Pierre Colombo , Chloe Clavel, Eric Gaussier, Giovanna Varni, Emmanuel Vignon, and Anne Sabourin.
Heavy-tailed representations, text polarity classification & data augmentation. NeurIPS 2020
[7] Alexandre Garcia,Pierre Colombo, Slim Essid, Florence d’Alché-Buc, and Chloé Clavel. From the token to the review: A
hierarchical multimodal approach to opinion mining. EMNLP 2020
[8] Pierre Colombo, Wojciech Witon, Ashutosh Modi, James Kennedy, and Mubbasir Kapadia. Affect-driven dialog generation.
NAACL 2019


